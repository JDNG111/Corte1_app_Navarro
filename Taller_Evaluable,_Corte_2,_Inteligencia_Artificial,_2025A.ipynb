{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfDqFAK1jDCoKUZpulYyZy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDNG111/Corte1_app_Navarro/blob/master/Taller_Evaluable%2C_Corte_2%2C_Inteligencia_Artificial%2C_2025A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taller Evaluable\n",
        "\n",
        "Corte 2\n",
        "\n",
        "Inteligencia Artificial\n",
        "\n",
        "2025A:\n",
        "\n",
        "***Hecho por: Julian David Navarro G.***\n",
        "\n",
        "Mayo 10 del 2025"
      ],
      "metadata": {
        "id": "Kda0BFOPKoIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 1: Configuración del Entorno y Carga de Modelo Base***\n",
        "\n",
        "Objetivo: Establecer el entorno de desarrollo necesario para trabajar con modelos LLM y cargar un modelo pre-entrenado utilizando las bibliotecas Transformers y PyTorch."
      ],
      "metadata": {
        "id": "Vu5TpvlZKtoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGU2I0eJ4pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2a8fc3-1eb3-4c0a-81f0-f413dffb367b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo 'ostorc/Conversational_Spanish_GPT' cargado en cpu.\n",
            "\n",
            "--- Respuesta generada ---\n",
            "Siento escuchar eso. Te mando muchos ánimos.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import os\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    # Carga el tokenizador y modelo\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    # Asegura que exista pad_token\n",
        "    if tokenizador.pad_token_id is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    modelo.eval()\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def main():\n",
        "    modelo_id = \"ostorc/Conversational_Spanish_GPT\"\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo(modelo_id)\n",
        "    print(f\"Modelo '{modelo_id}' cargado en {dispositivo}.\")\n",
        "\n",
        "    # Entrada del usuario + token EOS\n",
        "    entrada = \"estoy triste\"\n",
        "    input_ids = tokenizador.encode(entrada + tokenizador.eos_token, return_tensors=\"pt\").to(dispositivo)\n",
        "\n",
        "    # Generación: le decimos cuántos tokens más puede generar\n",
        "    with torch.no_grad():\n",
        "        chat_history = modelo.generate(\n",
        "            input_ids,\n",
        "            max_length=input_ids.shape[-1] + 50,      # hasta 50 tokens de respuesta\n",
        "            pad_token_id=tokenizador.eos_token_id\n",
        "        )\n",
        "\n",
        "    respuesta_ids = chat_history[:, input_ids.shape[-1]:][0]\n",
        "    respuesta = tokenizador.decode(respuesta_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(\"\\n--- Respuesta generada ---\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 2: Procesamiento de Entrada y Generación de Respuestas***\n",
        "\n",
        "Objetivo: Desarrollar las funciones necesarias para procesar la entrada del usuario, preparar los tokens para el modelo y generar respuestas coherentes."
      ],
      "metadata": {
        "id": "kE9DCj3_LEJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "    modelo.eval()\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(texto, tokenizador, dispositivo, longitud_maxima=512):\n",
        "    tokens = tokenizador.encode(\n",
        "        texto,\n",
        "        truncation=True,\n",
        "        max_length=longitud_maxima,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return tokens.to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada_procesada, tokenizador, parametros_generacion=None):\n",
        "    if parametros_generacion is None:\n",
        "        parametros_generacion = {\n",
        "            \"max_new_tokens\": 100,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"top_k\": 50,\n",
        "            \"num_return_sequences\": 1,\n",
        "            \"pad_token_id\": tokenizador.eos_token_id\n",
        "        }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        salida = modelo.generate(\n",
        "            entrada_procesada,\n",
        "            **parametros_generacion\n",
        "        )\n",
        "\n",
        "    respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
        "    return respuesta\n",
        "\n",
        "def crear_prompt_sistema(instrucciones, pregunta_usuario):\n",
        "    return f\"{instrucciones}\\nUsuario: {pregunta_usuario}\\nAsistente:\"\n",
        "\n",
        "def interaccion_simple():\n",
        "    nombre_modelo = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo(nombre_modelo)\n",
        "\n",
        "    instrucciones = \"Answer as best you can\"\n",
        "    entrada_usuario = \"cuentame un chiste\"\n",
        "    prompt = crear_prompt_sistema(instrucciones, entrada_usuario)\n",
        "\n",
        "    entrada = preprocesar_entrada(prompt, tokenizador, dispositivo)\n",
        "    respuesta = generar_respuesta(modelo, entrada, tokenizador)\n",
        "\n",
        "    print(\"\\n--- Interacción con el chatbot ---\")\n",
        "    print(\"Entrada:\")\n",
        "    print(prompt)\n",
        "    print(\"\\nRespuesta generada:\")\n",
        "    print(respuesta)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interaccion_simple()\n"
      ],
      "metadata": {
        "id": "KaxxR6xtLFyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cd77cd-5938-44dd-bbb3-d330058d4ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Interacción con el chatbot ---\n",
            "Entrada:\n",
            "Answer as best you can\n",
            "Usuario: cuentame un chiste\n",
            "Asistente:\n",
            "\n",
            "Respuesta generada:\n",
            "Answer as best you can\n",
            "Usuario: cuentame un chiste\n",
            "Asistente: ¡Claro! Aquí tienes uno:\n",
            "\n",
            "¿Qué hace una paloma en un supermercado?\n",
            "\n",
            "En el supermercado, sufrirá un cambio de vida. ¡Con su nuevo gabinete, va a ser la palomita más popular del mundo! \n",
            "\n",
            "Espero que te guste. ¿Hay algo más que puedes decirme? No dudes en preguntar. ¡Estoy aquí para ayudarte! 🙌✨\n",
            "\n",
            "No te pierdas más chistes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 3: Manejo de Contexto Conversacional***\n",
        "\n",
        "Objetivo: Implementar un sistema para mantener el contexto de la conversación, permitiendo al chatbot recordar intercambios anteriores y responder coherentemente a conversaciones prolongadas."
      ],
      "metadata": {
        "id": "5PFVKbj8TI9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# === Utilidades comunes ===\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Configuramos el pad_token si no está definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo).eval()\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(prompt, tokenizador, dispositivo, longitud_maxima=1024):\n",
        "    return tokenizador(prompt, return_tensors=\"pt\", truncation=True, max_length=longitud_maxima).to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada, tokenizador, prompt, parametros=None):\n",
        "    if parametros is None:\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": 80,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizador.pad_token_id,\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        salida_ids = modelo.generate(**entrada, **parametros)\n",
        "\n",
        "    texto_generado = tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "    respuesta = texto_generado[len(prompt):].strip()\n",
        "    return respuesta or \"[Empty response]\"\n",
        "\n",
        "# === Gestor de contexto ===\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"System: {contenido}\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"User: {contenido}\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"Assistant: {contenido}\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversación.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud máxima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "            self.historial.pop(1)  # Preserva instrucciones iniciales del sistema\n",
        "\n",
        "# === Clase Chatbot ===\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementación de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo_id (str): Identificador del modelo en Hugging Face\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        self.modelo, self.tokenizador, self.dispositivo = cargar_modelo(modelo_id)\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema que en este caso le proporcionaremos\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Parámetros para la generación\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        entrada = preprocesar_entrada(prompt, self.tokenizador, self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada, self.tokenizador, prompt, parametros_generacion)\n",
        "\n",
        "        # 6. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        # 7. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# === Prueba de conversación ===\n",
        "\n",
        "def prueba_conversacion():\n",
        "    \"\"\"\n",
        "    Función para probar el chatbot con una conversación de varios turnos.\n",
        "    \"\"\"\n",
        "    # Crear una instancia del chatbot con instrucciones del sistema\n",
        "    instrucciones = \"You are a helpful AI assistant that remembers the user's name and provides long, thoughtful answers.\"\n",
        "    chatbot = Chatbot(\"Qwen/Qwen2.5-0.5B-Instruct\", instrucciones)\n",
        "\n",
        "    # Simular una conversación de varios turnos\n",
        "    preguntas = [\n",
        "        \"Mi nombre es Julian\",\n",
        "        \"¿recuerdas cual es mi nombre?\", #Corroboramos de que el modelo es capaz de recordar datos en la conversación\n",
        "        \"Cuentame un dato curioso sobre la IA\",\n",
        "        \"¿puedes ayudarme a estudiar para un examen?\"\n",
        "    ]\n",
        "\n",
        "    for i, pregunta in enumerate(preguntas):\n",
        "        print(f\"\\n--- Turn {i + 1} ---\")\n",
        "        print(f\"User: {pregunta}\")\n",
        "        respuesta = chatbot.responder(pregunta)\n",
        "        print(f\"Assistant: {respuesta}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prueba_conversacion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e64MTvNUTICM",
        "outputId": "01c05a89-92a5-4312-cfb2-ff2fa620a9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando dispositivo: cpu\n",
            "\n",
            "--- Turn 1 ---\n",
            "User: Mi nombre es Alex\n",
            "Assistant: . ¿Cómo estás hoy? (Inglés)\n",
            "Assistant: Hola, Alex! Estoy muy bien, gracias por preguntar. ¿Cómo te encuentras hoy mismo? Me encanta interactuar con personas y compartir información sobre mí. ¿En qué puedo ayudarte hoy? ¡Por supuesto! No dudes en hacerme saber cómo puedo ayudarte más allá de lo que\n",
            "\n",
            "--- Turn 2 ---\n",
            "User: ¿recuerdas cual es mi nombre?\n",
            "Assistant: Assistant:\n",
            "\n",
            "Recuerdo que tu nombre es Alex. Como asistente digital, tengo la capacidad de recordar información hasta el último segundo. ¿Tienes alguna otra pregunta o necesitas ayuda con algo más? ¡Espero que tengas un buen día!\n",
            "\n",
            "--- Turn 3 ---\n",
            "User: Cuentame un dato curioso sobre la IA\n",
            "Assistant: .\n",
            "Assistant:\n",
            "\n",
            "¿Quién fue la primera persona a tener una conversación inteligente? La persona que llevó el primer intento de conversación inteligente en 1957. Aunque aún no conocemos quién era esa persona, su historia ha sido narrada y documentada en varios libros y películas. Este evento se llamó \"la conversación inteligente\" y\n",
            "\n",
            "--- Turn 4 ---\n",
            "User: ¿puedes ayudarme a estudiar para un examen?\n",
            "Assistant: Assistant:\n",
            "Como asistente, estoy aquí para proporcionar información útil y responder cualquier consulta que pueda tener. Para estudiar para un examen, hay algunas estrategias que puedes seguir:\n",
            "\n",
            "1. **Revisar previamente**: Antes de tomar el examen, asegúrate de revisar los temas que se presentarán en el examen. Puedes buscar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 4: Optimización del Modelo para Recursos Limitados***\n",
        "\n",
        "Objetivo: Implementar técnicas de optimización para mejorar la velocidad de inferencia y reducir el consumo de memoria, permitiendo que el chatbot funcione eficientemente en dispositivos con recursos limitados."
      ],
      "metadata": {
        "id": "daJ7dV7CUqaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    \"\"\"\n",
        "    Verifica y retorna el dispositivo disponible para ejecutar el modelo.\n",
        "\n",
        "    Returns:\n",
        "        torch.device: Dispositivo detectado (cuda o cpu)\n",
        "    \"\"\"\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def configurar_cuantizacion(bits=4):\n",
        "    \"\"\"\n",
        "    Configura los parámetros para la cuantización del modelo.\n",
        "\n",
        "    Args:\n",
        "        bits (int): Bits para cuantización (4 u 8)\n",
        "\n",
        "    Returns:\n",
        "        BitsAndBytesConfig: Configuración de cuantización\n",
        "    \"\"\"\n",
        "    if bits not in [4, 8]:\n",
        "        raise ValueError(\"La cuantización solo soporta 4 u 8 bits\")\n",
        "\n",
        "    # Configurar la cuantización utilizando BitsAndBytesConfig\n",
        "    config_cuantizacion = BitsAndBytesConfig(\n",
        "        load_in_4bit=bits == 4,\n",
        "        load_in_8bit=bits == 8,\n",
        "        bnb_4bit_quant_type=\"nf4\",  # Formato de cuantización (normal float 4 bits)\n",
        "        bnb_4bit_compute_dtype=torch.float16,  # Tipo de datos para cómputo\n",
        "        bnb_4bit_use_double_quant=True,  # Doble cuantización para ahorrar más memoria\n",
        "    )\n",
        "\n",
        "    return config_cuantizacion\n",
        "\n",
        "def cargar_modelo_optimizado(nombre_modelo, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Carga un modelo con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        nombre_modelo (str): Identificador del modelo\n",
        "        optimizaciones (dict): Diccionario con flags para las optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador, dispositivo)\n",
        "    \"\"\"\n",
        "    dispositivo = verificar_dispositivo()\n",
        "\n",
        "    if optimizaciones is None:\n",
        "        optimizaciones = {\n",
        "            \"cuantizacion\": torch.cuda.is_available(),  # Solo activar si hay GPU (ya que, al trabajar en Colab, no tenemos GPU)\n",
        "            \"bits\": 4,\n",
        "            \"offload_cpu\": False,\n",
        "            \"flash_attention\": torch.cuda.is_available()  # Solo activar si hay GPU\n",
        "        }\n",
        "\n",
        "    # Si estamos en CPU, desactivar optimizaciones que requieren GPU\n",
        "    if dispositivo.type == \"cpu\":\n",
        "        optimizaciones[\"cuantizacion\"] = False\n",
        "        optimizaciones[\"flash_attention\"] = False\n",
        "\n",
        "    # Preparar argumentos para cargar el modelo\n",
        "    model_args = {\n",
        "        \"pretrained_model_name_or_path\": nombre_modelo,\n",
        "        \"trust_remote_code\": True,\n",
        "    }\n",
        "\n",
        "    # Aplicar cuantización si está habilitada y hay GPU\n",
        "    if optimizaciones.get(\"cuantizacion\", False) and torch.cuda.is_available():\n",
        "        bits = optimizaciones.get(\"bits\", 4)\n",
        "        try:\n",
        "            model_args[\"quantization_config\"] = configurar_cuantizacion(bits)\n",
        "        except ImportError:\n",
        "            print(\"La librería bitsandbytes no está disponible. Desactivando cuantización.\")\n",
        "            optimizaciones[\"cuantizacion\"] = False\n",
        "\n",
        "    # Configurar offloading a CPU si está habilitado\n",
        "    if optimizaciones.get(\"offload_cpu\", False):\n",
        "        if torch.cuda.is_available():\n",
        "            model_args[\"device_map\"] = \"auto\"\n",
        "            model_args[\"offload_folder\"] = \"offload_folder\"\n",
        "            os.makedirs(\"offload_folder\", exist_ok=True)\n",
        "\n",
        "    # Configurar atención flash si está habilitada y disponible en hardware\n",
        "    if optimizaciones.get(\"flash_attention\", False) and torch.cuda.is_available():\n",
        "        try:\n",
        "            model_args[\"use_flash_attention_2\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"Flash Attention no pudo ser habilitada: {e}\")\n",
        "            optimizaciones[\"flash_attention\"] = False\n",
        "\n",
        "    # Cargar el tokenizador\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo, trust_remote_code=True)\n",
        "\n",
        "    # Configurar pad_token si no está definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    # Para CPUs, usar optimizaciones de CPU\n",
        "    if dispositivo.type == \"cpu\":\n",
        "        try:\n",
        "            model_args[\"torch_dtype\"] = torch.float16\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Cargar el modelo con las optimizaciones\n",
        "    try:\n",
        "        print(f\"Cargando modelo con optimizaciones: {optimizaciones}\")\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(**model_args)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo con optimizaciones. Intentando cargar sin optimizaciones: {e}\")\n",
        "        # Intento de respaldo: cargar modelo sin optimizaciones\n",
        "        backup_args = {\n",
        "            \"pretrained_model_name_or_path\": nombre_modelo,\n",
        "            \"trust_remote_code\": True\n",
        "        }\n",
        "        modelo = AutoModelForCausalLM.from_pretrained(**backup_args)\n",
        "\n",
        "    # Mover el modelo al dispositivo (si no se usa device_map='auto')\n",
        "    if not optimizaciones.get(\"offload_cpu\", False):\n",
        "        modelo = modelo.to(dispositivo)\n",
        "\n",
        "    # Establecer el modo de evaluación\n",
        "    modelo.eval()\n",
        "\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def aplicar_sliding_window(modelo, window_size=1024):\n",
        "    \"\"\"\n",
        "    Configura la atención de ventana deslizante para procesar secuencias largas.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a configurar\n",
        "        window_size (int): Tamaño de la ventana de atención\n",
        "    \"\"\"\n",
        "    # Verificar que el modelo tenga configuración de atención\n",
        "    try:\n",
        "        if hasattr(modelo.config, \"sliding_window\"):\n",
        "            # Configurar sliding window attention\n",
        "            modelo.config.sliding_window = window_size\n",
        "            modelo.config.use_sliding_window = True\n",
        "            print(f\"Sliding window configurada con tamaño {window_size}\")\n",
        "        else:\n",
        "            print(\"Este modelo no soporta sliding window attention de forma directa\")\n",
        "\n",
        "            # Intento alternativo para modelos que no tienen el atributo directamente\n",
        "            try:\n",
        "                for layer in modelo.model.layers:\n",
        "                    if hasattr(layer.self_attn, \"sliding_window\"):\n",
        "                        layer.self_attn.sliding_window = window_size\n",
        "                print(f\"Sliding window configurada manualmente con tamaño {window_size}\")\n",
        "            except:\n",
        "                print(\"No se pudo configurar sliding window para este modelo\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al configurar sliding window: {e}\")\n",
        "\n",
        "def aplicar_optimizaciones_cpu(modelo):\n",
        "    \"\"\"\n",
        "    Aplica optimizaciones específicas para CPU.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a optimizar\n",
        "    \"\"\"\n",
        "    # Fusión de operaciones cuando sea posible\n",
        "    try:\n",
        "        for module in modelo.modules():\n",
        "            if isinstance(module, nn.Sequential):\n",
        "                # Intentar fusionar operaciones secuenciales\n",
        "                torch.jit.script(module)\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo aplicar fusión de operaciones: {e}\")\n",
        "\n",
        "    try:\n",
        "        if hasattr(modelo, \"half\"):\n",
        "            modelo = modelo.half()\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo convertir a half precision: {e}\")\n",
        "\n",
        "    return modelo\n",
        "\n",
        "def medir_uso_memoria():\n",
        "    \"\"\"\n",
        "    Mide el uso actual de memoria.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (uso de RAM en MB, uso de VRAM en MB si disponible)\n",
        "    \"\"\"\n",
        "    # Medimos el uso de RAM\n",
        "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)  # MB\n",
        "\n",
        "    # Medimos el uso de VRAM si hay GPU disponible\n",
        "    vram_usage = 0\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        vram_usage = torch.cuda.memory_allocated() / (1024 * 1024)  # MB\n",
        "\n",
        "    return ram_usage, vram_usage\n",
        "\n",
        "def evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo):\n",
        "    \"\"\"\n",
        "    Evalúa el rendimiento del modelo en términos de velocidad y memoria.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a evaluar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        texto_prueba (str): Texto para pruebas de rendimiento\n",
        "        dispositivo: Dispositivo donde se ejecutará\n",
        "\n",
        "    Returns:\n",
        "        dict: Métricas de rendimiento\n",
        "    \"\"\"\n",
        "    # Asegurar que no hay cálculos pendientes en la GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Limpiar caché para medición más precisa\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Guardar uso de memoria antes de la inferencia\n",
        "    ram_antes, vram_antes = medir_uso_memoria()\n",
        "\n",
        "    # Preparar la entrada\n",
        "    inputs = tokenizador(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
        "    input_tokens = inputs.input_ids.shape[1]\n",
        "\n",
        "    # Calentar el modelo con una inferencia inicial\n",
        "    with torch.no_grad():\n",
        "        _ = modelo.generate(**inputs, max_new_tokens=10)\n",
        "\n",
        "    # Medir tiempo de inferencia (promedio de 3 ejecuciones)\n",
        "    tiempo_total = 0\n",
        "    num_ejecuciones = 3\n",
        "    output_tokens = 0\n",
        "\n",
        "    for _ in range(num_ejecuciones):\n",
        "        inicio = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = modelo.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95\n",
        "            )\n",
        "        fin = time.time()\n",
        "\n",
        "        # Acumular tiempo y tokens generados\n",
        "        tiempo_total += (fin - inicio)\n",
        "        output_tokens = outputs.shape[1] - input_tokens  # Tokens generados (excluye entrada)\n",
        "\n",
        "    # Calcular promedios\n",
        "    tiempo_inferencia = tiempo_total / num_ejecuciones\n",
        "    tokens_por_segundo = output_tokens / tiempo_inferencia\n",
        "\n",
        "    # Medir uso de memoria después de la inferencia\n",
        "    ram_despues, vram_despues = medir_uso_memoria()\n",
        "\n",
        "    # Calcular métricas\n",
        "    metricas = {\n",
        "        \"tiempo_inferencia_segundos\": tiempo_inferencia,\n",
        "        \"tokens_generados\": output_tokens,\n",
        "        \"tokens_por_segundo\": tokens_por_segundo,\n",
        "        \"uso_ram_mb\": ram_despues - ram_antes,\n",
        "        \"uso_vram_mb\": vram_despues - vram_antes if torch.cuda.is_available() else 0,\n",
        "        \"dispositivo\": str(dispositivo)\n",
        "    }\n",
        "\n",
        "    return metricas\n",
        "\n",
        "def mostrar_metricas(nombre, metricas):\n",
        "    \"\"\"\n",
        "    Muestra las métricas de rendimiento de forma legible.\n",
        "\n",
        "    Args:\n",
        "        nombre (str): Nombre de la configuración\n",
        "        metricas (dict): Métricas de rendimiento\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Métricas para {nombre} ---\")\n",
        "    print(f\"• Dispositivo: {metricas['dispositivo']}\")\n",
        "    print(f\"• Tiempo de inferencia: {metricas['tiempo_inferencia_segundos']:.4f} segundos\")\n",
        "    print(f\"• Tokens generados: {metricas['tokens_generados']}\")\n",
        "    print(f\"• Velocidad: {metricas['tokens_por_segundo']:.2f} tokens/segundo\")\n",
        "    print(f\"• Uso de RAM: {metricas['uso_ram_mb']:.2f} MB\")\n",
        "\n",
        "    if metricas['uso_vram_mb'] > 0:\n",
        "        print(f\"• Uso de VRAM: {metricas['uso_vram_mb']:.2f} MB\")\n",
        "\n",
        "def demo_optimizaciones():\n",
        "    \"\"\"\n",
        "    Demuestra y compara diferentes configuraciones de optimización.\n",
        "    \"\"\"\n",
        "    # Texto de prueba (suficientemente largo para evaluar rendimiento)\n",
        "    texto_prueba = \"\"\"\n",
        "    La inteligencia artificial (IA) es un campo de la informática que se centra en la creación de\n",
        "    máquinas capaces de imitar comportamientos inteligentes. Incluye subcampos como el aprendizaje\n",
        "    automático, el procesamiento del lenguaje natural, la visión por computadora y la robótica.\n",
        "    En los últimos años, hemos visto avances significativos en modelos de lenguaje que pueden\n",
        "    comprender y generar texto similar al humano. Estos modelos tienen aplicaciones en asistentes\n",
        "    virtuales, traducción, resumen de textos y muchas otras áreas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Configuraciones a probar (usamos un modelo pequeño para facilitar las pruebas)\n",
        "    modelo_base = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Elegimos un modelo pequeño de la familia Qwen\n",
        "    dispositivo = verificar_dispositivo()\n",
        "\n",
        "    # Adaptar las configuraciones según el dispositivo disponible\n",
        "    configs_a_probar = []\n",
        "\n",
        "    # 1. Modelo base sin optimizaciones (funciona en CPU y GPU)\n",
        "    configs_a_probar.append({\n",
        "        \"nombre\": \"Modelo Base\",\n",
        "        \"optimizaciones\": {\n",
        "            \"cuantizacion\": False,\n",
        "            \"flash_attention\": False,\n",
        "            \"offload_cpu\": False\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # 2. Para GPU: Añadir configuraciones que requieren GPU\n",
        "    if dispositivo.type == \"cuda\":\n",
        "        # Modelo con cuantización de 4 bits\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con cuantización 4-bit\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": True,\n",
        "                \"bits\": 4,\n",
        "                \"flash_attention\": False,\n",
        "                \"offload_cpu\": False\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Modelo con Flash Attention\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con Flash Attention\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": False,\n",
        "                \"flash_attention\": True,\n",
        "                \"offload_cpu\": False\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Modelo con todas las optimizaciones\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo con todas las optimizaciones\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": True,\n",
        "                \"bits\": 4,\n",
        "                \"flash_attention\": True,\n",
        "                \"offload_cpu\": True\n",
        "            }\n",
        "        })\n",
        "    # 3. Para CPU: Añadir configuraciones específicas para CPU\n",
        "    else:\n",
        "        # Modelo con optimizaciones específicas para CPU\n",
        "        configs_a_probar.append({\n",
        "            \"nombre\": \"Modelo optimizado para CPU (half precision)\",\n",
        "            \"optimizaciones\": {\n",
        "                \"cuantizacion\": False,\n",
        "                \"flash_attention\": False,\n",
        "                \"offload_cpu\": False,\n",
        "                \"half_precision\": True\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # 4. Modelo con sliding window (funciona en CPU y GPU)\n",
        "    configs_a_probar.append({\n",
        "        \"nombre\": \"Modelo con Sliding Window\",\n",
        "        \"optimizaciones\": {\n",
        "            \"cuantizacion\": False,\n",
        "            \"flash_attention\": False,\n",
        "            \"offload_cpu\": False,\n",
        "            \"sliding_window\": True\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Ejecutar pruebas para cada configuración\n",
        "    resultados = []\n",
        "\n",
        "    for config in configs_a_probar:\n",
        "        print(f\"\\n=== {config['nombre']} ===\")\n",
        "        try:\n",
        "            modelo, tokenizador, dispositivo = cargar_modelo_optimizado(\n",
        "                modelo_base,\n",
        "                optimizaciones=config[\"optimizaciones\"]\n",
        "            )\n",
        "\n",
        "            # Aplicar sliding window si está especificado\n",
        "            if config[\"optimizaciones\"].get(\"sliding_window\", False):\n",
        "                aplicar_sliding_window(modelo, window_size=256)\n",
        "\n",
        "            # Aplicar optimizaciones específicas para CPU si estamos en CPU\n",
        "            if dispositivo.type == \"cpu\" and config[\"optimizaciones\"].get(\"half_precision\", False):\n",
        "                modelo = aplicar_optimizaciones_cpu(modelo)\n",
        "\n",
        "            # Evaluar rendimiento\n",
        "            metricas = evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo)\n",
        "            mostrar_metricas(config[\"nombre\"], metricas)\n",
        "            resultados.append((config[\"nombre\"], metricas))\n",
        "\n",
        "            # Limpiar memoria\n",
        "            del modelo\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al probar {config['nombre']}: {e}\")\n",
        "\n",
        "    # Comparar resultados si hay más de una configuración exitosa\n",
        "    if len(resultados) > 1:\n",
        "        print(\"\\n=== Comparación de Rendimiento ===\")\n",
        "\n",
        "        # Usar el primer resultado como base para la comparación\n",
        "        base_nombre, base_metricas = resultados[0]\n",
        "        base_tiempo = base_metricas[\"tiempo_inferencia_segundos\"]\n",
        "        base_tokens = base_metricas[\"tokens_por_segundo\"]\n",
        "        base_ram = base_metricas[\"uso_ram_mb\"]\n",
        "        base_vram = base_metricas[\"uso_vram_mb\"]\n",
        "\n",
        "        print(\"\\nMejora relativa (comparado con modelo base):\")\n",
        "        for nombre, met in resultados:\n",
        "            if nombre == base_nombre:\n",
        "                continue\n",
        "\n",
        "            speedup = base_tiempo / met[\"tiempo_inferencia_segundos\"] if met[\"tiempo_inferencia_segundos\"] > 0 else 0\n",
        "            tokens_mejora = met[\"tokens_por_segundo\"] / base_tokens if base_tokens > 0 else 0\n",
        "            ram_reduccion = base_ram / met[\"uso_ram_mb\"] if met[\"uso_ram_mb\"] > 0 else 1\n",
        "\n",
        "            print(f\"\\n• {nombre}:\")\n",
        "            print(f\"  - Velocidad: {speedup:.2f}x más rápido\")\n",
        "            print(f\"  - Throughput: {tokens_mejora:.2f}x más tokens/segundo\")\n",
        "            print(f\"  - Uso de RAM: {ram_reduccion:.2f}x más eficiente\")\n",
        "\n",
        "            if torch.cuda.is_available() and base_vram > 0 and met[\"uso_vram_mb\"] > 0:\n",
        "                vram_reduccion = base_vram / met[\"uso_vram_mb\"] if met[\"uso_vram_mb\"] > 0 else 1\n",
        "                print(f\"  - Uso de VRAM: {vram_reduccion:.2f}x más eficiente\")\n",
        "    else:\n",
        "        print(\"\\nNo hay suficientes configuraciones exitosas para comparar.\")\n",
        "\n",
        "# Clase GestorContexto básica para integración con el chatbot\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes, optimizado para modelos Qwen.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"<|im_start|>system\\n{contenido}<|im_end|>\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"<|im_start|>user\\n{contenido}<|im_end|>\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"<|im_start|>assistant\\n{contenido}<|im_end|>\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversación.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud máxima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "\n",
        "            self.historial.pop(1)  # Guarda las instrucciones iniciales del sistema\n",
        "\n",
        "# Clase Chatbot adaptada para usar modelo optimizado\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementación de chatbot con manejo de contexto y optimizaciones.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo, tokenizador, dispositivo, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot con componentes pre-cargados.\n",
        "\n",
        "        Args:\n",
        "            modelo: Modelo de lenguaje pre-cargado\n",
        "            tokenizador: Tokenizador pre-cargado\n",
        "            dispositivo: Dispositivo (CPU/GPU)\n",
        "            instrucciones_sistema (str): Instrucciones iniciales\n",
        "        \"\"\"\n",
        "        self.modelo = modelo\n",
        "        self.tokenizador = tokenizador\n",
        "        self.dispositivo = dispositivo\n",
        "\n",
        "        # Crear gestor de contexto\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta optimizada al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Parámetros para la generación\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # Preparar parámetros de generación\n",
        "        if parametros_generacion is None:\n",
        "            parametros_generacion = {\n",
        "                \"max_new_tokens\": 150,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"do_sample\": True,\n",
        "                \"repetition_penalty\": 1.1,\n",
        "                \"pad_token_id\": self.tokenizador.pad_token_id,\n",
        "            }\n",
        "\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        if not \"<|im_start|>assistant\" in prompt:\n",
        "            prompt += \"\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        entrada = self.tokenizador(prompt, return_tensors=\"pt\", truncation=True).to(self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        with torch.no_grad():\n",
        "            salida_ids = self.modelo.generate(\n",
        "                **entrada,\n",
        "                **parametros_generacion\n",
        "            )\n",
        "\n",
        "        # 6. Decodificar la respuesta\n",
        "        texto_generado = self.tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "        respuesta = texto_generado[len(prompt):].strip()\n",
        "\n",
        "        # 7. Limpiar la respuesta\n",
        "        respuesta = respuesta.replace(\"#AIHelp\", \"\").strip()\n",
        "        if respuesta.startswith(\"Assistant:\"):\n",
        "            respuesta = respuesta[len(\"Assistant:\"):].strip()\n",
        "\n",
        "        # 8. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        return respuesta or \"[Empty response]\"\n",
        "\n",
        "# Creamos el chatbot optimizado con el modelo específico\n",
        "def crear_chatbot_optimizado(modelo_id, instrucciones_sistema=None, optimizaciones=None):\n",
        "    \"\"\"\n",
        "    Crea una instancia de chatbot con optimizaciones aplicadas.\n",
        "\n",
        "    Args:\n",
        "        modelo_id (str): ID del modelo en Hugging Face\n",
        "        instrucciones_sistema (str): Instrucciones iniciales del sistema\n",
        "        optimizaciones (dict): Configuración de optimizaciones\n",
        "\n",
        "    Returns:\n",
        "        Chatbot: Instancia del chatbot optimizado\n",
        "    \"\"\"\n",
        "    # Cargar modelo optimizado\n",
        "    modelo, tokenizador, dispositivo = cargar_modelo_optimizado(modelo_id, optimizaciones)\n",
        "\n",
        "    # Aplicar sliding window si está especificado\n",
        "    if optimizaciones and optimizaciones.get(\"sliding_window\", False):\n",
        "        aplicar_sliding_window(modelo, window_size=256)\n",
        "\n",
        "    # Aplicar optimizaciones específicas para CPU si estamos en CPU\n",
        "    if dispositivo.type == \"cpu\" and optimizaciones and optimizaciones.get(\"half_precision\", False):\n",
        "        modelo = aplicar_optimizaciones_cpu(modelo)\n",
        "\n",
        "    # Crear chatbot\n",
        "    chatbot = Chatbot(modelo, tokenizador, dispositivo, instrucciones_sistema)\n",
        "\n",
        "    return chatbot\n",
        "\n",
        "# Ejecutamos\n",
        "if __name__ == \"__main__\":\n",
        "    demo_optimizaciones()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktgM79XbVxh8",
        "outputId": "b0694a96-aac8-4738-aaeb-50916449fa86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando dispositivo: cpu\n",
            "\n",
            "=== Modelo Base ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False}\n",
            "\n",
            "--- Métricas para Modelo Base ---\n",
            "• Dispositivo: cpu\n",
            "• Tiempo de inferencia: 21.1676 segundos\n",
            "• Tokens generados: 50\n",
            "• Velocidad: 2.36 tokens/segundo\n",
            "• Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Modelo optimizado para CPU (half precision) ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False, 'half_precision': True}\n",
            "\n",
            "--- Métricas para Modelo optimizado para CPU (half precision) ---\n",
            "• Dispositivo: cpu\n",
            "• Tiempo de inferencia: 21.7830 segundos\n",
            "• Tokens generados: 50\n",
            "• Velocidad: 2.30 tokens/segundo\n",
            "• Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Modelo con Sliding Window ===\n",
            "Utilizando dispositivo: cpu\n",
            "Cargando modelo con optimizaciones: {'cuantizacion': False, 'flash_attention': False, 'offload_cpu': False, 'sliding_window': True}\n",
            "Sliding window configurada con tamaño 256\n",
            "\n",
            "--- Métricas para Modelo con Sliding Window ---\n",
            "• Dispositivo: cpu\n",
            "• Tiempo de inferencia: 21.7837 segundos\n",
            "• Tokens generados: 50\n",
            "• Velocidad: 2.30 tokens/segundo\n",
            "• Uso de RAM: 0.00 MB\n",
            "\n",
            "=== Comparación de Rendimiento ===\n",
            "\n",
            "Mejora relativa (comparado con modelo base):\n",
            "\n",
            "• Modelo optimizado para CPU (half precision):\n",
            "  - Velocidad: 0.97x más rápido\n",
            "  - Throughput: 0.97x más tokens/segundo\n",
            "  - Uso de RAM: 1.00x más eficiente\n",
            "\n",
            "• Modelo con Sliding Window:\n",
            "  - Velocidad: 0.97x más rápido\n",
            "  - Throughput: 0.97x más tokens/segundo\n",
            "  - Uso de RAM: 1.00x más eficiente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio 5: Personalización del Chatbot y Despliegue***\n",
        "\n",
        "Objetivo: Implementar técnicas para personalizar el comportamiento del chatbot y prepararlo para su despliegue como una aplicación web simple."
      ],
      "metadata": {
        "id": "T6AoBIyLYSbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necesario para ejecutar la web del ejercicio 5\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77idSFGEYnzk",
        "outputId": "ef9319b4-db05-4026-8329-5bdc839dfeca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# === Utilidades comunes ===\n",
        "\n",
        "def verificar_dispositivo():\n",
        "    dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
        "    return dispositivo\n",
        "\n",
        "def cargar_modelo(nombre_modelo):\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
        "\n",
        "    # Configurar pad_token si no está definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo).eval()\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "def preprocesar_entrada(prompt, tokenizador, dispositivo, longitud_maxima=1024):\n",
        "    return tokenizador(prompt, return_tensors=\"pt\", truncation=True, max_length=longitud_maxima).to(dispositivo)\n",
        "\n",
        "def generar_respuesta(modelo, entrada, tokenizador, prompt, parametros=None):\n",
        "    if parametros is None:\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": 80,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": tokenizador.pad_token_id,\n",
        "        }\n",
        "    with torch.no_grad():\n",
        "        salida_ids = modelo.generate(**entrada, **parametros)\n",
        "\n",
        "    texto_generado = tokenizador.decode(salida_ids[0], skip_special_tokens=True)\n",
        "    respuesta = texto_generado[len(prompt):].strip()\n",
        "    return respuesta or \"[Empty response]\"\n",
        "\n",
        "# === Gestor de contexto ===\n",
        "\n",
        "class GestorContexto:\n",
        "    \"\"\"\n",
        "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
        "        \"\"\"\n",
        "        Inicializa el gestor de contexto.\n",
        "\n",
        "        Args:\n",
        "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto\n",
        "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None)\n",
        "        \"\"\"\n",
        "        self.historial = []\n",
        "        self.longitud_maxima = longitud_maxima\n",
        "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
        "\n",
        "    def _formato_predeterminado(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Formato predeterminado para mensajes.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "\n",
        "        Returns:\n",
        "            str: Mensaje formateado\n",
        "        \"\"\"\n",
        "        if rol == \"sistema\":\n",
        "            return f\"System: {contenido}\"\n",
        "        elif rol == \"usuario\":\n",
        "            return f\"User: {contenido}\"\n",
        "        elif rol == \"asistente\":\n",
        "            return f\"Assistant: {contenido}\"\n",
        "        return contenido\n",
        "\n",
        "    def agregar_mensaje(self, rol, contenido):\n",
        "        \"\"\"\n",
        "        Agrega un mensaje al historial de conversación.\n",
        "\n",
        "        Args:\n",
        "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
        "            contenido (str): Contenido del mensaje\n",
        "        \"\"\"\n",
        "        self.historial.append((rol, contenido))\n",
        "\n",
        "    def construir_prompt_completo(self):\n",
        "        \"\"\"\n",
        "        Construye un prompt completo basado en el historial.\n",
        "\n",
        "        Returns:\n",
        "            str: Prompt completo para el modelo\n",
        "        \"\"\"\n",
        "        return \"\\n\".join([self.formato_mensaje(rol, cont) for rol, cont in self.historial])\n",
        "\n",
        "    def truncar_historial(self, tokenizador):\n",
        "        \"\"\"\n",
        "        Trunca el historial si excede la longitud máxima.\n",
        "\n",
        "        Args:\n",
        "            tokenizador: Tokenizador del modelo\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            prompt = self.construir_prompt_completo()\n",
        "            input_ids = tokenizador(prompt, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
        "            if input_ids.shape[1] <= self.longitud_maxima or len(self.historial) <= 1:\n",
        "                break\n",
        "            self.historial.pop(1)  # Preserva instrucciones iniciales del sistema\n",
        "\n",
        "# === Clase Chatbot ===\n",
        "\n",
        "class Chatbot:\n",
        "    \"\"\"\n",
        "    Implementación de chatbot con manejo de contexto.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modelo, tokenizador, dispositivo, instrucciones_sistema=None):\n",
        "        \"\"\"\n",
        "        Inicializa el chatbot.\n",
        "\n",
        "        Args:\n",
        "            modelo: Modelo cargado\n",
        "            tokenizador: Tokenizador del modelo\n",
        "            dispositivo: Dispositivo de procesamiento (CPU/GPU)\n",
        "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
        "        \"\"\"\n",
        "        self.modelo = modelo\n",
        "        self.tokenizador = tokenizador\n",
        "        self.dispositivo = dispositivo\n",
        "        self.gestor_contexto = GestorContexto()\n",
        "\n",
        "        # Inicializar el contexto con instrucciones del sistema\n",
        "        if instrucciones_sistema:\n",
        "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
        "\n",
        "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
        "        \"\"\"\n",
        "        Genera una respuesta al mensaje del usuario.\n",
        "\n",
        "        Args:\n",
        "            mensaje_usuario (str): Mensaje del usuario\n",
        "            parametros_generacion (dict): Parámetros para la generación\n",
        "\n",
        "        Returns:\n",
        "            str: Respuesta del chatbot\n",
        "        \"\"\"\n",
        "        # 1. Agregar mensaje del usuario al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
        "\n",
        "        # 2. Truncar el historial si es necesario\n",
        "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
        "\n",
        "        # 3. Construir el prompt completo\n",
        "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
        "\n",
        "        # 4. Preprocesar la entrada\n",
        "        entrada = preprocesar_entrada(prompt, self.tokenizador, self.dispositivo)\n",
        "\n",
        "        # 5. Generar la respuesta\n",
        "        respuesta = generar_respuesta(self.modelo, entrada, self.tokenizador, prompt, parametros_generacion)\n",
        "\n",
        "        # 6. Agregar respuesta al contexto\n",
        "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta)\n",
        "\n",
        "        # 7. Devolver la respuesta\n",
        "        return respuesta\n",
        "\n",
        "# === Personalización con PEFT/LoRA ===\n",
        "\n",
        "def configurar_peft(modelo, r=8, lora_alpha=32):\n",
        "    \"\"\"\n",
        "    Configura el modelo para fine-tuning con PEFT/LoRA.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo base\n",
        "        r (int): Rango de adaptadores LoRA\n",
        "        lora_alpha (int): Escala alpha para LoRA\n",
        "\n",
        "    Returns:\n",
        "        modelo: Modelo adaptado para fine-tuning\n",
        "    \"\"\"\n",
        "    # Identificar automáticamente los módulos de atención basados en la arquitectura del modelo\n",
        "    target_modules = None\n",
        "\n",
        "    # Detectar tipo de modelo basado en atributos\n",
        "    if hasattr(modelo, \"gpt_neox\"):\n",
        "        # Para modelos tipo GPT-NeoX\n",
        "        target_modules = [\"attention.query_key_value\", \"attention.dense\"]\n",
        "    elif hasattr(modelo, \"transformer\"):\n",
        "        if hasattr(modelo.transformer, \"h\") and hasattr(modelo.transformer.h[0], \"attn\"):\n",
        "            # Para modelos GPT-2\n",
        "            target_modules = [\"attn.c_attn\", \"attn.c_proj\"]\n",
        "        elif hasattr(modelo.transformer, \"encoder\") and hasattr(modelo.transformer.encoder, \"layer\"):\n",
        "            # Para modelos tipo BERT/RoBERTa\n",
        "            target_modules = [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"]\n",
        "    elif hasattr(modelo, \"model\") and hasattr(modelo.model, \"decoder\"):\n",
        "        # Para modelos tipo T5\n",
        "        target_modules = [\"q\", \"v\"]\n",
        "\n",
        "    if not target_modules:\n",
        "        print(\"No se pudo detectar automáticamente los módulos objetivo, usando configuración genérica\")\n",
        "        target_modules = [\"query\", \"value\", \"key\", \"out_proj\", \"dense\"]\n",
        "\n",
        "    print(f\"Configurando LoRA con target_modules: {target_modules}\")\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=r,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "\n",
        "    modelo_peft = get_peft_model(modelo, lora_config)\n",
        "    modelo_peft.print_trainable_parameters()\n",
        "\n",
        "    return modelo_peft\n",
        "\n",
        "def guardar_modelo(modelo, tokenizador, ruta):\n",
        "    \"\"\"\n",
        "    Guarda el modelo y tokenizador en una ruta específica.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo a guardar\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        ruta (str): Ruta donde guardar\n",
        "    \"\"\"\n",
        "    modelo.save_pretrained(ruta)\n",
        "    tokenizador.save_pretrained(ruta)\n",
        "    print(f\"Modelo y tokenizador guardados en: {ruta}\")\n",
        "\n",
        "def cargar_modelo_personalizado(ruta):\n",
        "    \"\"\"\n",
        "    Carga un modelo personalizado desde una ruta específica.\n",
        "\n",
        "    Args:\n",
        "        ruta (str): Ruta del modelo\n",
        "\n",
        "    Returns:\n",
        "        tuple: (modelo, tokenizador, dispositivo)\n",
        "    \"\"\"\n",
        "    dispositivo = verificar_dispositivo()\n",
        "    modelo = AutoModelForCausalLM.from_pretrained(ruta)\n",
        "    tokenizador = AutoTokenizer.from_pretrained(ruta)\n",
        "\n",
        "    # Configurar pad_token si no está definido\n",
        "    if tokenizador.pad_token is None:\n",
        "        tokenizador.pad_token = tokenizador.eos_token\n",
        "        tokenizador.pad_token_id = tokenizador.eos_token_id\n",
        "\n",
        "    modelo.to(dispositivo)\n",
        "    return modelo, tokenizador, dispositivo\n",
        "\n",
        "# === Interfaz web con Gradio ===\n",
        "\n",
        "def crear_chatbot_con_memoria(modelo, tokenizador, dispositivo):\n",
        "    \"\"\"\n",
        "    Crea una instancia de chatbot con memoria de conversación.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo de lenguaje\n",
        "        tokenizador: Tokenizador del modelo\n",
        "        dispositivo: Dispositivo (CPU/GPU)\n",
        "\n",
        "    Returns:\n",
        "        Chatbot: Instancia del chatbot\n",
        "    \"\"\"\n",
        "    instrucciones = \"Eres un asistente virtual amable y servicial. Intenta dar respuestas útiles y detalladas a las preguntas del usuario.\"\n",
        "    return Chatbot(modelo, tokenizador, dispositivo, instrucciones_sistema=instrucciones)\n",
        "\n",
        "def crear_interfaz_web(chatbot, parametros_predefinidos=None):\n",
        "    \"\"\"\n",
        "    Crea una interfaz web para el chatbot usando Gradio.\n",
        "\n",
        "    Args:\n",
        "        chatbot: Instancia del chatbot\n",
        "        parametros_predefinidos: Parámetros de generación predefinidos\n",
        "\n",
        "    Returns:\n",
        "        gr.Interface: Interfaz de Gradio\n",
        "    \"\"\"\n",
        "    # Estado para almacenar el historial de la conversación\n",
        "    historial_chat = []\n",
        "\n",
        "    def responder_mensaje(mensaje, historial=None, temperatura=0.7, top_p=0.9, max_tokens=100):\n",
        "        if historial is None:\n",
        "            historial = []\n",
        "\n",
        "        # Configurar parámetros de generación personalizados\n",
        "        parametros = {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"temperature\": temperatura,\n",
        "            \"top_p\": top_p,\n",
        "            \"do_sample\": True,\n",
        "            \"pad_token_id\": chatbot.tokenizador.pad_token_id,\n",
        "        }\n",
        "\n",
        "        # Obtener respuesta del chatbot\n",
        "        respuesta = chatbot.responder(mensaje, parametros_generacion=parametros)\n",
        "\n",
        "        # Actualizar historial\n",
        "        historial.append((mensaje, respuesta))\n",
        "        return \"\", historial\n",
        "\n",
        "    with gr.Blocks(title=\"Chatbot Personalizado con Memoria\") as interfaz:\n",
        "        gr.Markdown(\"# 🤖 Chatbot Personalizado\")\n",
        "        gr.Markdown(\"Este chatbot utiliza un modelo de lenguaje personalizado con adaptadores LoRA y mantiene memoria de la conversación.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                chat_output = gr.Chatbot(height=400, label=\"Conversación\")\n",
        "                mensaje_input = gr.Textbox(placeholder=\"Escribe tu mensaje aquí...\", label=\"Mensaje\")\n",
        "                enviar_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Configuración\")\n",
        "                temperatura_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.05, label=\"Temperatura\")\n",
        "                top_p_slider = gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label=\"Top-p\")\n",
        "                max_tokens_slider = gr.Slider(minimum=10, maximum=200, value=100, step=10, label=\"Máx. tokens\")\n",
        "                clear_btn = gr.Button(\"Nueva conversación\")\n",
        "\n",
        "        # Eventos\n",
        "        enviar_btn.click(\n",
        "            fn=responder_mensaje,\n",
        "            inputs=[mensaje_input, chat_output, temperatura_slider, top_p_slider, max_tokens_slider],\n",
        "            outputs=[mensaje_input, chat_output]\n",
        "        )\n",
        "        mensaje_input.submit(\n",
        "            fn=responder_mensaje,\n",
        "            inputs=[mensaje_input, chat_output, temperatura_slider, top_p_slider, max_tokens_slider],\n",
        "            outputs=[mensaje_input, chat_output]\n",
        "        )\n",
        "        clear_btn.click(lambda: ([], []), outputs=[mensaje_input, chat_output])\n",
        "\n",
        "    return interfaz\n",
        "\n",
        "# === Despliegue ===\n",
        "\n",
        "def main_despliegue(modelo_base=\"Qwen/Qwen2.5-0.5B-Instruct\", usar_modelo_personalizado=False, ruta_modelo_personalizado=\"modelo_personalizado\"):\n",
        "    \"\"\"\n",
        "    Función principal para el despliegue del chatbot.\n",
        "\n",
        "    Args:\n",
        "        modelo_base (str): Identificador del modelo base en HuggingFace\n",
        "        usar_modelo_personalizado (bool): Si True, carga un modelo personalizado guardado\n",
        "        ruta_modelo_personalizado (str): Ruta donde se encuentra el modelo personalizado\n",
        "    \"\"\"\n",
        "    if usar_modelo_personalizado:\n",
        "        print(f\"Cargando modelo personalizado desde: {ruta_modelo_personalizado}\")\n",
        "        modelo, tokenizador, dispositivo = cargar_modelo_personalizado(ruta_modelo_personalizado)\n",
        "    else:\n",
        "        print(f\"Cargando modelo base: {modelo_base}\")\n",
        "        modelo, tokenizador, dispositivo = cargar_modelo(modelo_base)\n",
        "\n",
        "        # Como opción: Aplicar PEFT/LoRA al modelo base\n",
        "        aplicar_peft = False\n",
        "        if aplicar_peft:\n",
        "            print(\"Aplicando adaptadores LoRA al modelo base...\")\n",
        "            modelo = configurar_peft(modelo)\n",
        "\n",
        "            # Guardar el modelo personalizado\n",
        "            guardar_modelo(modelo, tokenizador, ruta_modelo_personalizado)\n",
        "            print(f\"Modelo personalizado guardado en: {ruta_modelo_personalizado}\")\n",
        "\n",
        "    # Crear instancia del chatbot\n",
        "    chatbot = crear_chatbot_con_memoria(modelo, tokenizador, dispositivo)\n",
        "\n",
        "    # Crear y lanzar la interfaz web\n",
        "    print(\"Iniciando la interfaz web...\")\n",
        "    interfaz = crear_interfaz_web(chatbot)\n",
        "    interfaz.launch(share=True)  # share=True permite acceso público a través de internet\n",
        "    print(\"Interfaz web iniciada.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Opciones de configuración\n",
        "    options = {\n",
        "        \"modelo_base\": \"Qwen/Qwen2.5-0.5B-Instruct\",  # Modelo base a utilizar\n",
        "        \"usar_modelo_personalizado\": False,  # Cambiar a True para cargar un modelo personalizado\n",
        "        \"ruta_modelo_personalizado\": \"modelo_personalizado\"  # Ruta del modelo personalizado\n",
        "    }\n",
        "\n",
        "    main_despliegue(**options)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "AadMCR0kYTUz",
        "outputId": "784cfd65-70c2-4d41-dca7-767eae4a9e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo base: Qwen/Qwen2.5-0.5B-Instruct\n",
            "Utilizando dispositivo: cpu\n",
            "Iniciando la interfaz web...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e2c36589aaad>:320: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat_output = gr.Chatbot(height=400, label=\"Conversación\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f53388deaee8008ece.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f53388deaee8008ece.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interfaz web iniciada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preguntas Teóricas***"
      ],
      "metadata": {
        "id": "wRJXBvfGVOB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*¿Cuáles son las diferencias fundamentales entre los modelos encoder-only, decoder-only y encoder-decoder en el contexto de los chatbots conversacionales? Explique qué tipo de modelo sería más adecuado para cada caso de uso y por qué.*"
      ],
      "metadata": {
        "id": "ABItK3PpVReZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Basicamente, los modelos encoder-only, (como por ejemplo el modelo BERT), se enfocan en entender texto, por lo que son ideales para tareas tales como de clasificación, detección de intención o análisis de sentimientos, pero no generan respuestas. Por otro lado, los decoder-only, tales como GPT, generan texto de forma autoregresiva y son los más adecuados para chatbots conversacionales abiertos, ya que pueden producir respuestas coherentes y lo más importante, es que lo hacen turno tras turno. Finalmente, los modelos encoder-decoder, (así como T5 o BART), combinan la comprensión y generación, siendo útiles para tareas tan importantes como traducción, resumen o respuestas basadas en un contexto cerrado. Esto lo que nos permite concluir, es que para chatbots de conversación abierta y continua, los decoder-only son una buena opción."
      ],
      "metadata": {
        "id": "WzNIFRm0Vdi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explique el concepto de \"temperatura\" en la generación de texto con LLMs. ¿Cómo afecta al comportamiento del chatbot y qué consideraciones debemos tener al ajustar este parámetro para diferentes aplicaciones?*"
      ],
      "metadata": {
        "id": "xFEC7R-sVT3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Como pudimos comprobar, la temperatura es un parámetro que regula la aleatoriedad en la generación de texto. Una temperatura baja (como por ejemplo una de 0.3) hace que el modelo sea más preciso y repetitivo, ideal para tareas donde se requiere confiabilidad y exactitud. Por el contrario, una temperatura alta (como por ejemplo una de 1.0) promueve en el bot más diversidad y creatividad, pero hay que tener en cuenta que esto puede llevar a errores o incoherencias. Así que, el ajustar este valor depende del uso y del contexto de desarrollo: en asistentes técnicos o educativos conviene usar temperaturas bajas, mientras que en aplicaciones creativas o de entretenimiento seria preferible el usar unas temperaturas más altas.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q7dXJLW6VsfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Describa las técnicas principales para reducir el problema de \"alucinaciones\" en chatbots basados en LLMs. ¿Qué estrategias podemos implementar a nivel de inferencia y a nivel de prompt engineering para mejorar la precisión factual de las respuestas?*"
      ],
      "metadata": {
        "id": "e-n8gFd_VZw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Respuesta:**\n",
        "\n",
        "Para mitigar alucinaciones (o tambien conocidas como respuestas incorrectas), se pueden aplicar técnicas tanto durante la inferencia como en el diseño del prompt. A nivel de inferencia, usar una temperatura baja, tales estrategias como el top-k/top-p sampling y modelos con recuperación de información externa (como por ejemplo RAG) ayuda a mejorar la precisión. Debemos tambien tener en cuanta que en cuanto al prompt engineering, dar instrucciones explícitas, incluir contexto relevante y utilizar ejemplos guía (que en ingles a esto se le conoce como few-shot prompting) nos puede ayudar a reducir errores. En ultima instancia, también es útil pedirle al modelo que diga “no sé” si no tiene certeza, en lugar de inventar una respuesta o simplemente, como se le llama a una \"alucinación\"."
      ],
      "metadata": {
        "id": "eq9pJ5ucV7j2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HECHO POR: Julian David Navarro G.\n",
        "\n",
        "IA - Mayo de 2025"
      ],
      "metadata": {
        "id": "PB7FnF5xWJtT"
      }
    }
  ]
}